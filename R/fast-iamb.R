
fast.incremental.association.optimized = function(x, whitelist, blacklist,
  test, alpha, B, strict, debug = FALSE) {

  nodes = names(x)
  mb2 = mb = list()

  # 1. [Compute Markov Blankets]
  for (node in nodes) {

    backtracking = unlist(sapply(mb, function(x){ node %in% x  }))

    mb[[node]] = fast.ia.markov.blanket(node, data = x, nodes = nodes,
         alpha = alpha, B = B, whitelist = whitelist, blacklist = blacklist,
         backtracking = backtracking, test = test, debug = debug)

  }#FOR

  # check markov blankets for consistency.
  mb = bn.recovery(mb, nodes = nodes, strict = strict, mb = TRUE, debug = debug)

  # 2. [Compute Graph Structure]
  for (node in nodes) {

    backtracking = unlist(sapply(mb2, function(x){ node %in% x[["nbr"]]  }))

    # save results in a copy of mb.
    mb2[[node]] = neighbour(node, mb = mb, data = x, alpha = alpha,
         B = B, whitelist = whitelist, blacklist = blacklist,
         backtracking = backtracking, test = test, debug = debug)

  }#FOR

  # update mb with the results of neighbour().
  mb = mb2

  # check neighbourhood sets for consistency.
  mb = bn.recovery(mb, nodes = nodes, strict = strict, debug = debug)

  return(mb)

}#FAST.INCREMENTAL.ASSOCIATION.OPTIMIZED

fast.incremental.association.cluster = function(x, cluster, whitelist,
  blacklist, test, alpha, B, strict, debug = FALSE) {

  nodes = names(x)

  # 1. [Compute Markov Blankets]
  mb = parLapply(cluster, as.list(nodes), fast.ia.markov.blanket,
         data = x, nodes = nodes, alpha = alpha, B = B, whitelist = whitelist,
         blacklist = blacklist, test = test, debug = debug)
  names(mb) = nodes

  # check markov blankets for consistency.
  mb = bn.recovery(mb, nodes = nodes, strict = strict, mb = TRUE, debug = debug)

  # 2. [Compute Graph Structure]
  mb = parLapply(cluster, as.list(nodes), neighbour, mb = mb, data = x,
         alpha = alpha, B = B, whitelist = whitelist, blacklist = blacklist,
         test = test, debug = debug)
  names(mb) = nodes

  # check neighbourhood sets for consistency.
  mb = bn.recovery(mb, nodes = nodes, strict = strict, debug = debug)

  return(mb)

}#FAST.INCREMENTAL.ASSOCIATION.CLUSTER

fast.incremental.association = function(x, whitelist, blacklist, test,
  alpha, B, strict, debug = FALSE) {

  nodes = names(x)

  # 1. [Compute Markov Blankets]
  mb = lapply(as.list(nodes), fast.ia.markov.blanket, data = x,
         nodes = nodes, alpha = alpha, B = B, whitelist = whitelist,
         blacklist = blacklist, test = test, debug = debug)
  names(mb) = nodes

  # check markov blankets for consistency.
  mb = bn.recovery(mb, nodes = nodes, strict = strict, mb = TRUE, debug = debug)

  # 2. [Compute Graph Structure]
  mb = lapply(as.list(nodes), neighbour, mb = mb, data = x, alpha = alpha,
         B = B, whitelist = whitelist, blacklist = blacklist, test = test,
         debug = debug)
  names(mb) = nodes

  # check neighbourhood sets for consistency.
  mb = bn.recovery(mb, nodes = nodes, strict = strict, debug = debug)

  return(mb)

}#FAST.INCREMENTAL.ASSOCIATION

fast.ia.markov.blanket = function(x, data, nodes, alpha, B, whitelist, blacklist,
  start = character(0), backtracking = NULL, test, debug = FALSE) {

  nodes = nodes[nodes != x]
  known.good = known.bad = c()
  whitelisted = nodes[sapply(nodes, function(y) {
    is.whitelisted(whitelist, c(x, y), either = TRUE) })]
  mb = start

  if (debug) {

    cat("----------------------------------------------------------------\n")
    cat("* learning the markov blanket of", x, ".\n")

    if (length(start) > 0)
      cat("* initial set includes '", mb, "'.\n")

  }#THEN

  # whitelisted nodes are included by default (if there's a direct arc
  # between them of course they are in each other's markov blanket).
  # arc direction is irrelevant here.
  mb = unique(c(mb, whitelisted))
  nodes = nodes[!(nodes %in% mb)]
  # blacklist is not checked, not all nodes in a markov blanket must be
  # neighbours.

  # use backtracking for a further screening of the nodes to be checked.
  if (!is.null(backtracking)) {

    # nodes whose markov blanket includes this node are included, because
    # X \in MB(Y) <=> Y \in MB(X)
    known.good = names(backtracking[backtracking])
    mb = unique(c(mb, known.good))

    # and vice versa X \not\in MB(Y) <=> Y \not\in MB(X)
    known.bad = names(backtracking[!backtracking])

    # both are not to be checked for inclusion/exclusion.
    nodes = nodes[!(nodes %in% names(backtracking))]

    if (debug) {

      cat("    * known good (backtracking): '", known.good, "'.\n")
      cat("    * known bad (backtracking): '", known.bad, "'.\n")
      cat("    * nodes still to be tested for inclusion: '", nodes, "'.\n")

    }#THEN

  }#THEN

  repeat {

    # reset the insufficient.data boolean flag.
    insufficient.data = FALSE

    # growing phase.
    if (debug)
      cat("  * checking nodes for association (growing phase).\n")

    # do not check the nodes in the markov blanket again.
    nodes = nodes[!(nodes %in% mb)]

    cands = c()
    pvalues = c()

    # speculatively add the node if the statistical tests is good
    for (node in nodes) {

      a = conditional.test(x=x, y=node, sx=mb, test = test, data = data, B = B,
                           alpha = alpha, debug = debug)

      if (a <= alpha) {

        cands = c(cands, node)
        pvalues = c(pvalues, a)

      }#THEN

    }#FOR

    # stop if there are no candidates for inclusion.
    if (length(cands) == 0)
      break

    # heuristic (1/2) : sort the candidates in increasing p-value order
    cands = cands[order(pvalues)]
    pvalues = pvalues[order(pvalues)]

    # add the candidates
    for (node in cands) {

      if (test %in% asymptotic.tests) {

        opc = obs.per.cell(x, node, mb, data)

        # do not add new nodes if that compromises the asymptotic behaviour
        # of the statistical test.
        if (opc < 5) {

          if (debug)
            cat("  @ not enough observations per cell (", opc ,"), skipping.\n")

          insufficient.data = TRUE
          break

        }#THEN
        else if (debug)

            cat("    @", node, "included in the markov blanket (obs/cell:", opc, ").\n")

      }#THEN
      else if (debug)

          cat("    @", node, "included in the markov blanket. ( p-value:", pvalues[node == cands], ")\n")

      mb = c(mb, node)

    }#FOR

    # shrinking phase
    if (debug)
      cat("  * checking nodes for exclusion (shrinking phase).\n")

    # whitelisted nodes are neighbours, they cannot be removed from the
    # markov blanket; speculatively adding nodes prevents further
    # optimizations.
    # known.good nodes from backtracking are not to be removed, either.
    to.check = mb[!(mb %in% c(known.good, whitelisted))]

    # heuristic (2/2) : order nodes from the lastly to the firstly added
    # this way we are more prone to remove less correlated nodes first
    if (length(to.check) > 0)
      to.check = rev(to.check)

    for (node in to.check) {

      if (length(mb) < 2) {

        if (debug)
          cat("  * markov blanket too small, no exclusion possible.")

        break

      }#THEN

      a = conditional.test(x=x, y=node, sx=mb[mb != node], data=data,
                           test=test, B=B, alpha=alpha, debug=debug)

      if (a > alpha) {

        if (debug)
          cat("    @ node", node, "removed from the markov blanket.\n")

        # update the markov blanket
        mb = mb[mb != node]

        # reset the insufficient.data boolean flag.
        insufficient.data = FALSE

      }#THEN

    }#FOR

    # if there are still not enough observations, stop iterating.
    if (insufficient.data)
      break

  }#REPEAT

  return(mb)

}#FAST.IA.MARKOV.BLANKET

